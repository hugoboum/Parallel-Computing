---
title: "Parallel Computing Project"
author: 'Hugo Brehier'
subtitle: 'Statistics for Smart Data'
date: '8/01/2020'
output:
  pdf_document:
    number_sections: yes
    df_print: paged
---

\pagebreak
\tableofcontents
\pagebreak


```{r include= FALSE}
knitr::opts_chunk$set(warning=FALSE, message=FALSE,fig.align="center")

```


# Data generation

The code is situated in datagen.R (function rlogit()).

We generate the independant variables  according to two normal distribution: $N(0,1)$ and  $N(1,2)$,
such that half the variables follow either one of the two distribution.
The response variable is then generated through the logit model.


# Auxiliary functions

The code is situated in aux_fcts_2b.R .
These function are the building blocks of the Newton-Rhapson algorithm, which we will see afterwards.
My source for the following functions is : \cite{ref1}.

## Negative Log-likelihood
It is the function loglikl() which computes the log-likelihood  $\ell(\theta)$ of the model.
In particular, we use its negative, $J(\theta) = -\ell(\theta)$.

## Gradient
It is the function gradient() which computes the gradient of the negative log-likelihood in relation to $\theta$,  $\nabla J(\theta)$.

## Hessian
It is the function hessian() which computes the Hessian of the negative log-likelihood in relation to $\theta$, $H_{J({\theta})}$


# Newton–Raphson algorithm

The algorithm is implemented in the last function of aux_fcts_2b.R (function rhapson_newton()).
This algorithm allows us to find a minimum of the negative log-likelihood $J(\theta)$ by the following repeated schema, until convergence: \newline
$\theta_{n+1} = \theta_{n} - H_{J({\theta})}^{-1}\nabla J(\theta)$ \newline
Thus, we indeed make use of the auxiliary functions.
See \cite{ref2} for further reference.

# Basic functions  
These functions are situated in basics.R .
They use of the Newton–Raphson algorithm and expand it further : cross-validation, model comparison and model selection. Each is a building block for the next.

## Cross-Validation
It is the function basic.cv(), which uses rhapson_newton().


## Model comparison
It is the function basic.modelcomparison(), which uses basic.cv().


## Model selection
It is the function basic.modelselection(), which uses basic.modelcomparison().


# Exceptions

Here is a list of some expections created :

* check the number of folds in basic.cv()
* warn of LOOCV in basic.cv()
* check the type entered in basic.nodelcomparison() variable *models*.


# Code profiling

Newton–Raphson algorithm is the core of further cross-validation and model selection.
Thus, this central piece is a priority to profile.

First result is in run1a.html. 
I detected the transpose operation in the Hessian function was the main consumer of time.
Thus, I stored it *outside the loop*. Result in run1b.html

Then I tried to replace the %*% operator with a custom one.

```{r}
library(Rcpp)
library(microbenchmark)
library(inline)
```

There are two ways to incorporate C++ functions.

```{r}

cppFunction('NumericMatrix mmult(const NumericMatrix& m1, const NumericMatrix& m2){
if (m1.ncol() != m2.nrow()) stop ("Incompatible matrix dimensions");
NumericMatrix out(m1.nrow(),m2.ncol());
NumericVector rm1, cm2;
for (size_t i = 0; i < m1.nrow(); ++i) {
    rm1 = m1(i,_);
    for (size_t j = 0; j < m2.ncol(); ++j) {
      cm2 = m2(_,j);
      out(i,j) = std::inner_product(rm1.begin(), rm1.end(), cm2.begin(), 0.);              
    }
  }
return out;
}')

```

```{r}
matmut <- cxxfunction(signature(tm="NumericMatrix",
                           tm2="NumericMatrix"),
                 plugin="RcppEigen",
                 body="
NumericMatrix tm22(tm2);
NumericMatrix tmm(tm);

const Eigen::Map<Eigen::MatrixXd> ttm(as<Eigen::Map<Eigen::MatrixXd> >(tmm));
const Eigen::Map<Eigen::MatrixXd> ttm2(as<Eigen::Map<Eigen::MatrixXd> >(tm22));

Eigen::MatrixXd prod = ttm*ttm2;
return(wrap(prod));
                 ")
```

We can benchmark them.

```{r}
set.seed(123)
M1 <- matrix(sample(1e3),ncol=50)
M2 <- matrix(sample(1e3),nrow=50)
```


```{r}
identical(matmut(M1,M2), M1 %*% M2)
```


```{r}
identical(mmult(M1,M2), M1 %*% M2)
```


```{r}
res <- microbenchmark(mmult(M1,M2),matmut(M1,M2), M1 %*% M2,times = 100000)
print(res)
```

matmut is faster. But for vectors, we need to cast them into matrix of length 1. It cancels out .
Rather, we impose the beta vector to be a 1D matrix from the beginning.
We also store the $x^Tx$ outside the loop in the hessian function.
We also use chol2inv(chol()) instead of solve() to inverse the hessian.
Results are in run3a.html \newline

After this, we vectorize computations in the auxiliary functions, which loop over each observation.
In run3a.html, I use sapply.
In run4a.html, I use lapply with standart dot product.
Unfortunately, it does not fasten the algorithm...
In run5a.html, I use cpp_lapply, another inline C++ function to mimic lapply.
It doesn't run faster.

Our best candidate is run2a.html , with loops, variables stored outside of loops and standart matrix multiplication.


# Parallel computing

In run6a.html, I use mclapply in the hessian function.
run6b.html should contain results where I use mclapply in the auxiliary functions.
Unfortunately, it was slow or an error occured since it did not finish.



# Consistency of the procedure of model selection

I finally changed the stopping criterion to the number of iterations rather than the norm of the direction used in \cite{ref2}. \newline
 
**run7.R contains the code for the study** with the profiled functions (datagen.R, aux_fcts_2b.R and basics.R). \newline

First, I checked that the algorithm got the same MLE as glm.
It was the case for 2 covariates, then it was the same results but scaled differently for 4 covariates. (by a factor of 10 for example).\newline
 
In my trial,both backward and forward selection recover the same model.
Moreover, they recovered the covariates with non-zero coefficients in the generation of data.

\pagebreak

# Conclusion

In this project, I implemented stepwise selection of a logit model estimated though MLE and CV.
Code profilling has shown that loops can be faster than vectorization.
Parallel computing sadly did not prove an improvement over vectorization *and* loops as  mclapply in the hessian function was slow.
The [**Iteratively reweighted least squares** method](https://en.wikipedia.org/wiki/Iteratively_reweighted_least_squares) or the [**BFGS method**](https://en.wikipedia.org/wiki/Broyden%E2%80%93Fletcher%E2%80%93Goldfarb%E2%80%93Shanno_algorithm) may prove to be faster.
But further code profilling should be considered.

*Thank you for your attention, happy new year ! *



\pagebreak
\bibliographystyle{apalike}
\bibliography{biblio}
